#   91视频爬虫

## 数据存储模型

91的首页有视频的总页数，现在目前是4000多页，并且在不断的更新，其中每页有20个视频。
首先为所有的数据建立缓存，本质开发效率快和不浪费服务器资源的想法。

由于页数是向后推迟的，如何保证页面的
### 缓存类型:

- disk_cache基于本地文件储存的缓存
- mongo_cache基于mongodb的缓存
其中每个类的都实现了内建的setitem,getitem,len方法。



### 抓取步骤

- 抓取4000多个网页,将Html储存在数据库中 db_name html_cache
- 抓取4000多个网页每页的20个视频url
- 抓取8万多个视频的real_url，也就是服务器的资源地址
- 抓取视频
上面的四个步骤通过4个方法来实现，并且通过在方法中传入collection的参数来动态的指定对应的collections


## mongo_cache

db_name 应该在什么时候指定

List所有的数据，{"_id":1,"key":"sdsd"}

        res=collection.find()
        for r in res[:1]:
            print(r)
如何确保定时抓取视频
4544 逆序抓取视频，下次再抓取视频时首先判断缓存中是否存在，
但还是不可行啊，意思对html页面的标识一定要有代表性，因为页面是可变的。\



## 需要改善的点

### 避免重复抓取

随着数量的增加，每个视频属于的主页Html_index也会发生偏移，如何针对这个点进行改善。
还有数据库中_id的选取，目前是拿每个item的url作为_id,item的value作为key。
视频的_id为视频的url,视频的key为视频的title。拿url作为_id的缺点，视频的url是可以变的，
url后面为视频的时间戳，这个点还需要改善。视频的value是否应该储存更多的信息还是只有title。
因为每个视频的信息是储存过一次的。
 

## 动态ip的功能

每个ip在未登录的情况下，只能抓取10个视频，如何设置ip代理，避免ip被封掉。对于登录的用户可以多看点视频。
如何进行登录。可能是通过cookie来记录登录状态的。这个还需要再学习下。

## 分布式任务队列以及异常监控

应该提供一个任务队列来处理抓取任务，失败的直接扔到队列尾部。如果出现异常，能否和centry结合起来。




## 爬虫的进一步展望

抓取虎扑上有意思的视频，和抖音的视频抓取。


